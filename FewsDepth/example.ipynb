{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from unidepth.utils import colorize, image_grid\n",
    "from unidepth.models import UniDepthV1\n",
    "from unidepth.utils.visualization import colorize_np\n",
    "from unidepth.utils.geometric import project_points\n",
    "from PIL import Image\n",
    "import open3d as o3d\n",
    "import cv2\n",
    "import fuse_depth as fd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiate: dinov2_vitl14\n",
      "0.9666967638737914\n",
      "0.9745800862325769\n",
      "1.0194454043313603\n",
      "1.0140443181375922\n"
     ]
    }
   ],
   "source": [
    "model = UniDepthV1.from_pretrained(\"lpiccinelli/unidepth-v1-vitl14\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "base_path = \"image_8.png\"\n",
    "H,W  = np.array(Image.open(base_path)).shape[:2]\n",
    "\n",
    "support_paths = []\n",
    "for filename in listdir(\"supports\"): \n",
    "    full_path = join(\"supports\", filename)\n",
    "    support_paths.append(full_path)\n",
    "\n",
    "\n",
    "sup_pcds = []\n",
    "for i, sup_path in enumerate(support_paths):\n",
    "    T, scale, torch_intrinsics1, torch_points1 = fd.pairwise_pose(base_path, sup_path, model) # base, sup order\n",
    "    if scale > 1.2 or scale < 0.9:\n",
    "        continue\n",
    "    pcd1 = o3d.io.read_point_cloud(\"ply2/im1.ply\")\n",
    "    pcd2 = o3d.io.read_point_cloud(\"ply2/im2.ply\")\n",
    "    pcd2 = pcd2.scale(scale, center=(0, 0, 0))\n",
    "    pcd2 = pcd2.transform(T)\n",
    "    o3d.io.write_point_cloud(f\"ply2/{i}.ply\", pcd2)\n",
    "    sup_pcds.append(pcd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def icp(pcd1, pcd2):\n",
    "    source = copy.deepcopy(pcd1)\n",
    "    source.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
    "    target = copy.deepcopy(pcd2)\n",
    "    target.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        source, target, 0.1, np.eye(4),\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=1000))\n",
    "    print(\"Transformation is:\")\n",
    "    print(reg_p2p.transformation)\n",
    "    return reg_p2p.transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation is:\n",
      "[[ 0.99997074  0.00322195 -0.00693759  0.04197298]\n",
      " [-0.0031981   0.99998895  0.00344604 -0.04152347]\n",
      " [ 0.00694861 -0.00342375  0.99997     0.05695842]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Transformation is:\n",
      "[[ 0.99970674 -0.00802815 -0.02284698  0.16833429]\n",
      " [ 0.00836004  0.99986037  0.01446861 -0.09075355]\n",
      " [ 0.02272764 -0.01465536  0.99963427  0.08234831]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Transformation is:\n",
      "[[ 9.99949587e-01  3.54623954e-03  9.39396956e-03 -9.06751107e-02]\n",
      " [-3.55210052e-03  9.99993507e-01  6.07297808e-04 -1.77446787e-02]\n",
      " [-9.39175494e-03 -6.40635516e-04  9.99955691e-01  1.71750380e-02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "Transformation is:\n",
      "[[ 0.99998818  0.00330423 -0.00356781  0.01028848]\n",
      " [-0.00326413  0.99993209  0.01118753 -0.06911063]\n",
      " [ 0.00360454 -0.01117575  0.99993105 -0.00282219]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "pcd_comb = copy.deepcopy(pcd1)\n",
    "for pcd in sup_pcds:\n",
    "    T = icp(pcd, pcd1)\n",
    "    dup_pcd = copy.deepcopy(pcd)\n",
    "    dup_pcd.transform(T)\n",
    "    pcd_comb += dup_pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_comb = fd.create_depth_map(np.asarray(pcd_comb.points), torch_intrinsics1.squeeze().cpu().numpy(), (H,W))\n",
    "depth_comb = cv2.medianBlur(depth_comb.astype('float32'), 5)\n",
    "\n",
    "depth_col_avg = colorize_np(depth_comb, vmin=0.01, vmax=10.0, cmap=\"magma_r\")\n",
    "Image.fromarray(depth_col_avg).save(\"median.png\")\n",
    "\n",
    "points_tensor = torch.tensor(np.asarray(pcd_comb.points), dtype=torch.float)\n",
    "points_tensor = points_tensor.view(-1, 3).unsqueeze(0).cuda()\n",
    "torch_depth_mean = project_points(points_tensor, torch_intrinsics1, (H,W))\n",
    "depth_pred_col_mean = colorize(torch_depth_mean.squeeze().cpu().numpy(), vmin=0.01, vmax=10.0, cmap=\"magma_r\")\n",
    "Image.fromarray(depth_pred_col_mean).save(\"mean.png\")\n",
    "\n",
    "point_cloud_flat = torch_points1.view(1, 3, -1)  # Shape: (B, 3, HW)\n",
    "point_cloud_transposed = point_cloud_flat.transpose(1, 2)  # Shape: (B, HW, 3)\n",
    "torch_depth = project_points(point_cloud_transposed, torch_intrinsics1, (H,W))\n",
    "depth_pred_col = colorize(torch_depth.squeeze().cpu().numpy(), vmin=0.01, vmax=10.0, cmap=\"magma_r\")\n",
    "Image.fromarray(depth_pred_col).save(\"original.png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = np.load(\"COS429_Final/data/environment1/depths/depth_8.npy\")\n",
    "gt_col = colorize_np(gt, vmin=0.01, vmax=10.0, cmap=\"magma_r\")\n",
    "Image.fromarray(gt_col).save(\"gt.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_depth = torch_depth.squeeze().cpu().numpy()\n",
    "mean_depth = torch_depth_mean.squeeze().cpu().numpy()\n",
    "\n",
    "og_depth[np.isnan(gt)] = 0\n",
    "depth_comb[np.isnan(gt)] = 0\n",
    "mean_depth[np.isnan(gt)] = 0 \n",
    "gt[np.isnan(gt)] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Reconstructed Depth MSE: 6.1231437\n",
      "Mean Reconstructed Depth MSE: 6.1895876\n",
      "Original Depth MSE: 6.098673\n"
     ]
    }
   ],
   "source": [
    "mse = np.mean((depth_comb - gt) ** 2)\n",
    "print(\"Median Reconstructed Depth MSE:\", mse)\n",
    "\n",
    "mse = np.mean((mean_depth - gt) ** 2)\n",
    "print(\"Mean Reconstructed Depth MSE:\", mse)\n",
    "\n",
    "mse = np.mean((og_depth - gt) ** 2)\n",
    "print(\"Original Depth MSE:\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('unidepth')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3caef7f33b181344cf27749f296d184d39308c97946686da309cd29485f94e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
